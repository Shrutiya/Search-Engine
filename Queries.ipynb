{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Queries.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFboIGSXdfyk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92c51e37-dd62-487e-9654-19eb295a473b"
      },
      "source": [
        "!pip install autocorrect"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autocorrect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/71/eb8c1f83439dfe6cbe1edb03be1f1110b242503b61950e7c292dd557c23e/autocorrect-2.2.2.tar.gz (621kB)\n",
            "\r\u001b[K     |▌                               | 10kB 17.9MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 21.8MB/s eta 0:00:01\r\u001b[K     |█▋                              | 30kB 16.8MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 11.6MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51kB 7.9MB/s eta 0:00:01\r\u001b[K     |███▏                            | 61kB 8.1MB/s eta 0:00:01\r\u001b[K     |███▊                            | 71kB 7.8MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81kB 8.3MB/s eta 0:00:01\r\u001b[K     |████▊                           | 92kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 102kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 112kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 122kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 133kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 143kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 153kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 163kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 174kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 184kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 194kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 204kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 215kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 225kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 235kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 245kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 256kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 266kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 276kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 286kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 296kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 307kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 317kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 327kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 337kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 348kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 358kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 368kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 378kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 389kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 399kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 409kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 419kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 430kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 440kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 450kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 460kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 471kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 481kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 491kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 501kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 512kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 522kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 532kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 542kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 552kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 563kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 573kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 583kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 593kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 604kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 614kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 624kB 8.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.2.2-cp36-none-any.whl size=621491 sha256=f4a96e0597b43fdd70cd7e79e4a68ef4d374ab79686a56d357139add98acb408\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/0b/7d/98268d64c8697425f712c897265394486542141bbe4de319d6\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAHMXAiUv054"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import json\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import time\n",
        "from tabulate import tabulate\n",
        "from autocorrect import Speller"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGOGj4LlCqVm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9760a060-3722-4c9a-9712-b44267751505"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YduoVHwticEi"
      },
      "source": [
        "f=open('drive/My Drive/AIR/inverted_index1.json','r')\n",
        "inverted_index=json.loads(f.read())\n",
        "f=open('drive/My Drive/AIR/record_list.json','r')\n",
        "record_list=json.loads(f.read())\n",
        "f=open('drive/My Drive/AIR/permuterm1.json','r')\n",
        "permuterm_index=json.loads(f.read())\n",
        "f=open('drive/My Drive/AIR/snippet_df.txt','r')\n",
        "snippet_df=eval(f.read())\n",
        "doc_location='drive/My Drive/archive/TelevisionNews/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2z4C6f5jlvD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a01215ce-01b2-4d73-b3e9-a2d21f071052"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stopWords = stopwords.words(\"english\")\n",
        "stopWords.append(\"i\\'ve\")\n",
        "stopWords.append(\"i\\'m\")\n",
        "stopWords.append(\"he\\'d\")\n",
        "stopWords.append(\"she\\'d\")\n",
        "stopWords.append(\"i\\'d\")\n",
        "stopWords.append(\"i\\'ll\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:          \n",
        "        return None\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_sentence = []\n",
        "    for word in sentence.split():\n",
        "            lemmatized_sentence.append(lemmatizer.lemmatize(word))\n",
        "    return \" \".join(lemmatized_sentence)\n",
        "\n",
        "def special_characters_removal(word):\n",
        "  special=list(string.punctuation)\n",
        "  special.append(' ')\n",
        "  special.remove('*')\n",
        "  x=[]\n",
        "  for i in word:\n",
        "    if i not in special:\n",
        "      x.append(i)\n",
        "    else:\n",
        "      x.append(' ')\n",
        "  return ''.join(x) \n",
        "\n",
        "def preprocessing(snippet):\n",
        "  snippet=snippet.split()\n",
        "  res=[]\n",
        "  for i in range(len(snippet)):\n",
        "    if snippet[i] not in stopWords and len(snippet[i])>1:\n",
        "      res.extend(special_characters_removal(snippet[i]).split())\n",
        "  snippet=\" \".join(res)\n",
        "  snippet=lemmatize_sentence(snippet)\n",
        "  return snippet.lower()  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7ainrEvkU60"
      },
      "source": [
        "## **`Boolean Queries`**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY9cQLV9kRh0"
      },
      "source": [
        "def mergeand(left_operand,right_operand):\n",
        "  doc_list=set(left_operand.keys()).intersection(set(right_operand.keys()))\n",
        "  final_record_list={}\n",
        "  for k in doc_list:\n",
        "    x=left_operand[k].intersection(right_operand[k])\n",
        "    if x:\n",
        "      final_record_list[k]=x\n",
        "  return final_record_list\n",
        "\n",
        "def mergeor(left_operand,right_operand):\n",
        "  doc_list=set(left_operand.keys()).union(set(right_operand.keys()))\n",
        "  final_record_list={k:left_operand[k].union(right_operand[k] if k in right_operand else set()) if k in left_operand else right_operand[k] for k in doc_list}\n",
        "  return final_record_list\n",
        "\n",
        "def get_not_posting_list(operand):\n",
        "  files=set(record_list.keys())\n",
        "  post_list={k:set(map(str,range(record_list[k][0]))).difference(operand[k] if k in operand else {}) for k in files}\n",
        "  return post_list\n",
        "\n",
        "def show_records(final):\n",
        "  result=[]\n",
        "  #result.insert(0,'Document Name',pd.Series)\n",
        "  #result.insert(1,'Row Number',int)\n",
        "  for i in final:\n",
        "    doc_l=doc_location+i\n",
        "    df=pd.read_csv(doc_l)\n",
        "    for j in final[i]:\n",
        "      result.append(df.loc[int(j)])\n",
        "      #result['Document Name'].loc[len(result)-1]=i\n",
        "  return result\n",
        "\n",
        "def get_posting_list(term):\n",
        "  if term not in inverted_index:\n",
        "    return dict()\n",
        "  x=list(inverted_index[term].keys())\n",
        "  x.remove('doc_frequency')\n",
        "  post_list={k:set(inverted_index[term][k]['records'].keys()) for k in x}\n",
        "  return post_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDGsUDH29pFZ"
      },
      "source": [
        "def parsebooleanquery(query):\n",
        "  query=query.replace('(',' ( ')\n",
        "  query=query.replace(')',' ) ')\n",
        "  precedence={}\n",
        "  precedence['NOT']=3\n",
        "  precedence['OR']=2\n",
        "  precedence['AND']=1\n",
        "  precedence['(']=0\n",
        "  precedence[')']=0\n",
        "  operand_stack=[]\n",
        "  operator_stack=[]\n",
        "  postfix_stack=[]\n",
        "  j=0\n",
        "  l=query.split()\n",
        "  while(j<len(l)):\n",
        "    i=l[j]\n",
        "    if i==\"(\":\n",
        "      operator_stack.append(i)\n",
        "    elif i==\")\":\n",
        "      while( operator_stack and operator_stack[-1]!=\"(\" ):\n",
        "        postfix_stack.append(operator_stack.pop())\n",
        "      operator_stack.pop()\n",
        "    elif i in precedence:\n",
        "      while(operator_stack and precedence[operator_stack[-1]]>precedence[i]):\n",
        "        postfix_stack.append(operator_stack.pop())\n",
        "      operator_stack.append(i)\n",
        "    else:\n",
        "      x=i\n",
        "      j+=1\n",
        "      while(j<len(l) and l[j] not in precedence):\n",
        "        x=x+\" \"+l[j]\n",
        "        j+=1\n",
        "      if not operator_stack or operator_stack[-1]!=\"NOT\":\n",
        "        operand_stack.append(x)\n",
        "      postfix_stack.append(x)\n",
        "      j-=1\n",
        "    j+=1\n",
        "  while(operator_stack):\n",
        "    postfix_stack.append(operator_stack.pop())\n",
        "  #print(postfix_stack,operand_stack)\n",
        "  return postfix_stack,operand_stack\n",
        "\n",
        "def eval_query(postfix_stack):\n",
        "  result=[]\n",
        "  for i in postfix_stack:\n",
        "    if i==\"AND\":\n",
        "      left_operand=result.pop()\n",
        "      right_operand=result.pop()\n",
        "      result.append(mergeand(left_operand,right_operand))\n",
        "    elif i==\"OR\":\n",
        "      left_operand=result.pop()\n",
        "      right_operand=result.pop()\n",
        "      result.append(mergeor(left_operand,right_operand))\n",
        "    elif i==\"NOT\":\n",
        "      operand=result.pop()\n",
        "      result.append(get_not_posting_list(operand))\n",
        "    else:\n",
        "      result.append(parsephrasequery(i) if '*' not in i else eval_wildcard_query(i))\n",
        "  return result[0]\n",
        "\n",
        "def get_results(query):\n",
        "  p_stack,o_stack=parsebooleanquery(query)\n",
        "  return eval_query(p_stack),o_stack"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqdon_QJeH_q"
      },
      "source": [
        "# Phrase query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhFn1RAWixj4"
      },
      "source": [
        "def parsephrasequery(query):\n",
        "  query=preprocessing(spellcheck(query))\n",
        "  tokens=query.split()\n",
        "  intermediate_result=get_posting_list(tokens[0])\n",
        "  for i in range(1,len(tokens)):\n",
        "    intermediate_result=mergeand(intermediate_result,get_posting_list(tokens[i]))\n",
        "  final_record_list=mergephrase(tokens,intermediate_result) if len(tokens)>1 else intermediate_result\n",
        "  return final_record_list\n",
        "\n",
        "def get_positional_list(doc_name, record_no, term):\n",
        "  return inverted_index[term][doc_name]['records'][record_no]['position'] \n",
        "\n",
        "def mergephrase(tokens,intersect_list):\n",
        "  final_record_list={}\n",
        "  for doc in intersect_list:\n",
        "    records=intersect_list[doc]\n",
        "    for record in records:\n",
        "      temp_pos=[]\n",
        "      for i in range(len(tokens)):\n",
        "        temp_pos.append(set(map(lambda x:x-i,get_positional_list(doc, record, tokens[i]))))\n",
        "      if (set(temp_pos[0].intersection(*temp_pos))):\n",
        "        if (doc not in final_record_list):\n",
        "          final_record_list[doc]=set()\n",
        "        final_record_list[doc].add(record)\n",
        "  return final_record_list  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJwLRUHlPe4T"
      },
      "source": [
        "# Wildcard Queries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr6lzEgAPmBa"
      },
      "source": [
        "def splitquery(query):\n",
        "  query1=query.split(\"*\")\n",
        "  if query1[0]=='' and len(query1)==3 and query1[2]=='':\n",
        "    return [query]\n",
        "  elif len(query1)==2 and (query1[0]=='' or  query1[1]==''):\n",
        "    return [query]\n",
        "  else:\n",
        "    result=[]\n",
        "    if query1[0]=='':\n",
        "      result.append(\"*\"+query1[1]+\"*\")\n",
        "      i=2\n",
        "    else:\n",
        "      result.append(query1[0]+\"*\")\n",
        "      i=1\n",
        "    while(i<len(query1)-1):\n",
        "      result.append(\"*\"+query1[i]+\"*\")\n",
        "      i+=1\n",
        "    if query1[i]!='':\n",
        "      result.append(\"*\"+query1[i])\n",
        "    return result\n",
        "  \n",
        "def permuterm_form(query):\n",
        "  query=query.split('*')\n",
        "  if query[0]=='' and len(query)==3 and query[2]=='': #*X*\n",
        "    actual_query=query[1]\n",
        "  elif query[0]=='' and len(query)==2:  #*X\n",
        "    actual_query=query[1]+\"$\"\n",
        "  elif query[1]=='' and len(query)==2:  #X*\n",
        "    actual_query=\"$\"+query[0]\n",
        "  return actual_query\n",
        "\n",
        "def parsewildcardquery(input_query):\n",
        "  x=splitquery(input_query)\n",
        "  result=get_wildcard_result(permuterm_form(x[0]))\n",
        "  for query in x[1:]:\n",
        "    result=result.intersection(get_wildcard_result(permuterm_form(query)))\n",
        "  tokens=input_query.split('*')\n",
        "  result1=[]\n",
        "  #print(tokens,result)\n",
        "  for i in result:\n",
        "    y=i\n",
        "    flag=0\n",
        "    for j in tokens:\n",
        "      if j=='':\n",
        "        pass\n",
        "      else:\n",
        "        pos=y.find(j)\n",
        "        if pos==-1:\n",
        "          flag=1\n",
        "          break\n",
        "        else:\n",
        "          y=y[pos+len(j):]\n",
        "    if flag==0:\n",
        "      result1.append(i)\n",
        "  return result1\n",
        "\n",
        "def get_wildcard_result(query):\n",
        "  result=[]\n",
        "  for i in permuterm_index:\n",
        "    if i.startswith(query):\n",
        "      result.append(permuterm_index[i])\n",
        "  return set(result)\n",
        "\n",
        "def eval_wildcard_query(query):\n",
        "  x=parsewildcardquery(query)\n",
        "  #print(x)\n",
        "  result=get_posting_list(x[0])\n",
        "  for i in x[1:]:\n",
        "    result=mergeor(get_posting_list(i),result)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW8bQaUgNCLt"
      },
      "source": [
        "# Ranking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukNO6WWRQcup"
      },
      "source": [
        "preprocessed_snippet_df=list(map(lambda x:preprocessing(x[0]),snippet_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6h69gzmX-ui"
      },
      "source": [
        "def ranked_results(query,k):\n",
        "  f=open('text2.txt','w')\n",
        "  t1=time.time()\n",
        "  results,tokens=get_results(query)\n",
        "  if not results:\n",
        "    return \"No results obtained.\"\n",
        "  tokens1=[]\n",
        "  for i in range(len(tokens)):\n",
        "    if '*' in tokens[i]:\n",
        "      tokens1.extend(list(parsewildcardquery(tokens[i])))\n",
        "    else:\n",
        "      tokens1.append(tokens[i])\n",
        "  #print(tokens1)\n",
        "  filtered_tfidf=[]\n",
        "  snippets=[]\n",
        "  for i in results:\n",
        "    for j in results[i]:\n",
        "      filtered_tfidf.append(record_list[i][1]+int(j))\n",
        "      snippets.append(preprocessed_snippet_df[filtered_tfidf[-1]])\n",
        "  results1=[]\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  X = vectorizer.fit_transform(snippets)\n",
        "  query_vec = vectorizer.transform([preprocessing(\" \".join(tokens1))])\n",
        "  #for i in range(len(filtered_tfidf)):\n",
        "  results1=cosine_similarity(X,query_vec).reshape((-1,))\n",
        "  results2=sorted(range(len(results1)),reverse=True, key=lambda x:results1[x])\n",
        "  t2=time.time()\n",
        "  print()\n",
        "  k=min(k,len(results2))\n",
        "  print(len(results2),\" results fetched in \",t2-t1,\"seconds. Top \",k,\"are being shown below - \")\n",
        "  print()\n",
        "  table=[]\n",
        "  for i in results2[:k]:\n",
        "      x=snippet_df[filtered_tfidf[i]]\n",
        "      table.append([x[1],x[2],x[0]])\n",
        "  print(tabulate(table,headers=[\"Document Name\",\"Row number\",\"Snippet\"]))\n",
        "  print()\n",
        "  for i in results2:\n",
        "      f.write(snippet_df[filtered_tfidf[i]][0])\n",
        "      f.write(\"\\n\")\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI4Rpe5IKcTx"
      },
      "source": [
        "# Spelling correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ASgsvsTdWV7"
      },
      "source": [
        "def spellcheck(query):\n",
        "  spell = Speller(lang='en')\n",
        "  query1=query\n",
        "  query = query.split()\n",
        "  corrected=[]\n",
        "  for i in range(len(query)):\n",
        "    if query[i] in inverted_index:\n",
        "      corrected.append(query[i])\n",
        "    else:\n",
        "      corrected.append(spell(query[i]))\n",
        "  corrected=\" \".join(corrected)\n",
        "  if corrected!=query1:\n",
        "    print(\"Did you mean\",corrected,\"?\")\n",
        "    #print(query,corrected)\n",
        "  return corrected"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RdVeANx4kyn"
      },
      "source": [
        "# Mixed Queries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjh67xNrd4KB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9882976e-2a99-4b53-a303-02181be1ea68"
      },
      "source": [
        "ranked_results('(carbon dioxide OR co-2) AND methane AND NOT climate',16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "43  results fetched in  0.34451746940612793 seconds. Top  16 are being shown below - \n",
            "\n",
            "Document Name         Row number  Snippet\n",
            "------------------  ------------  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "CNN.201405.csv                59  the permafrost, if it thaws, there's going to be a methane release that's even worse than carbon dioxide. methane in the air will produce more of the greenhouse gas and greenhouse warming than even co 2. the oceans are absorbing this co-2. they're becoming acidic.\n",
            "MSNBC.201909.csv             244  sulfur dioxide in the basin exceed nationally mandated standards. a recent study found methane trapped heat at a rate more than 80 times that of co 2, accounting for a quarter of current global warming but this is texas, a state where the energy companies tell the regulators how much they are\n",
            "MSNBC.201402.csv             134  perhaps gone down. a lot of that was due to the recession. right. methane emissions are expl e exploding. we're looking at the total greenhouse gas impact, right, not just the co 2. the natural gas industrial we say, we burn cleaner than coal. they're not reporting the fact\n",
            "MSNBC.200912.csv             321  it's not whether or not we're going through a global warming period. we were. we're not. now god is still up there. we're now going through a cooling spell. the whole issue was is it mandate gases? co 2, methane? i don't think so.\n",
            "BBCNEWS.202001.csv           297  heat trapping gas, that is why people talk about carbon dioxide as a greenhouse gas, the effect is like you get the warmth inside a greenhouse, that is why carbon dioxide and other gases, which are known to have the same kind of property, like methane for example,\n",
            "BBCNEWS.202001.csv           324  asa as a heat trapping gas, that is why people talk about carbon dioxide as a greenhouse gas, the effect is like you're getting the warmth inside a greenhouse so that is why carbon dioxide and other gases which are known to have the same kind of property like methane for example,\n",
            "BBCNEWS.201906.csv             4  because we were emitting carbon dioxide and methane into the air at a runaway rate, but what i hadn't fully understood is this - simply reducing greenhouse gas emissions will not bring global warming under control.\n",
            "MSNBC.201306.csv              79  105 times more than carbon dioxide does. you need 80 to 105 pounds of carbon dioxide to equal one pound of methane. to get the same greenhouse gas effect. right. which means you need something like any more than 1% leakage of methane in the total production of natural gas means that you're\n",
            "BBCNEWS.201909.csv           200  and we have been breaking records in main greenhouse gas concentrations, carbon dioxide, which is the most important one and also methane and nitrous oxide. those are all striking. and we have\n",
            "FOXNEWS.201811.csv            62  you're also getting a lot of methane produced. and methane is a much more active global warming gas than carbon dioxide. so there's a lot of problems associated with the permafrost. be when you're talking about melting that has occurred.\n",
            "MSNBC.201402.csv              18  this study, researchers estimate the country's methane emissions are probably about 50% higher than the epa, your own agency, currently believes. that's important because methane is a potent greenhouse gas with 30 times the global warming potential of carbon dioxide.\n",
            "BBCNEWS.201906.csv            20  happening because we were omitting carbon dioxide and methane into the airata air at a runaway rate, but what they hadn't fully understood is this. simply reducing greenhouse gas emissions will not bring global\n",
            "BBCNEWS.201906.csv             2  global warming was happening because we were emitting carbon dioxide and methane into the air at a runaway rate, but what i hadn't fully understood is this - simply reducing greenhouse gas emissions will not bring global warming under control.\n",
            "MSNBC.201402.csv               1  the country's methane emissions are probably about 50% higher than the epa, your own agency, currently believes. that's important because methane is a potent greenhouse gas with 30 times the global warming potential of carbon dioxide. are you going to take another look at what contribution fracking is having to global\n",
            "BBCNEWS.201906.csv           408  now, i'd known that our weather was getting worse and our sea levels were rising, and i'd known that global warming was happening because we were emitting carbon dioxide and methane into the air at a runaway rate, but what i hadn't fully understood is this -  _\n",
            "BBCNEWS.201906.csv           409  now, i'd known that our weather was getting worse and our sea levels were rising, and i'd known that global warming was happening because we were emitting carbon dioxide and methane into the air at a runaway rate, but what i hadn't fully understood is this -  _\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE6uiMoMLUug"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}